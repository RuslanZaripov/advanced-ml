{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.14","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"!pip install -q gdown","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2024-11-18T20:32:11.419848Z","iopub.execute_input":"2024-11-18T20:32:11.420166Z","iopub.status.idle":"2024-11-18T20:32:21.760364Z","shell.execute_reply.started":"2024-11-18T20:32:11.420117Z","shell.execute_reply":"2024-11-18T20:32:21.759122Z"}},"outputs":[],"execution_count":1},{"cell_type":"code","source":"# download model\n!gdown 1-1VRanRyciw8IOMnAoE9p7ftvpWRNS7w","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# download dataset\n!gdown 1sqaarcZFTvB2mGVTwpSOc_srNSw2GgCQ --output transformer_data.zip","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-18T20:32:21.762476Z","iopub.execute_input":"2024-11-18T20:32:21.762796Z","iopub.status.idle":"2024-11-18T20:32:37.090317Z","shell.execute_reply.started":"2024-11-18T20:32:21.762765Z","shell.execute_reply":"2024-11-18T20:32:37.089286Z"}},"outputs":[{"name":"stdout","text":"Downloading...\nFrom (original): https://drive.google.com/uc?id=1sqaarcZFTvB2mGVTwpSOc_srNSw2GgCQ\nFrom (redirected): https://drive.google.com/uc?id=1sqaarcZFTvB2mGVTwpSOc_srNSw2GgCQ&confirm=t&uuid=aaa03b6e-16b2-407d-9a92-3f8480c7cacd\nTo: /kaggle/working/transformer_data.zip\n100%|████████████████████████████████████████| 799M/799M [00:10<00:00, 78.6MB/s]\n","output_type":"stream"}],"execution_count":2},{"cell_type":"code","source":"!pip install -q youtokentome wget sacrebleu","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-18T20:32:37.092217Z","iopub.execute_input":"2024-11-18T20:32:37.092706Z","iopub.status.idle":"2024-11-18T20:33:05.358242Z","shell.execute_reply.started":"2024-11-18T20:32:37.092648Z","shell.execute_reply":"2024-11-18T20:33:05.357248Z"}},"outputs":[],"execution_count":3},{"cell_type":"markdown","source":"# Imports","metadata":{}},{"cell_type":"code","source":"import torch\nfrom torch import nn\nfrom torch.nn import functional as F\nfrom torch.nn.utils.rnn import pad_sequence\nfrom torch.utils.data import DataLoader\n\nimport logging\n\nimport math\nimport os\nimport tarfile\nimport wget\nimport shutil\nimport itertools\nimport time\nimport codecs\nfrom tqdm import tqdm\nfrom datetime import date\nimport sacrebleu\n\nfrom random import shuffle\nimport youtokentome as yttm\n\nfrom itertools import groupby","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-18T20:34:53.514624Z","iopub.execute_input":"2024-11-18T20:34:53.515135Z","iopub.status.idle":"2024-11-18T20:34:53.520973Z","shell.execute_reply.started":"2024-11-18T20:34:53.515099Z","shell.execute_reply":"2024-11-18T20:34:53.519925Z"}},"outputs":[],"execution_count":5},{"cell_type":"markdown","source":"# Download data","metadata":{}},{"cell_type":"code","source":"def flatten(destination):\n    all_files = []\n    for root, _dirs, files in itertools.islice(os.walk(destination), 1, None):\n        for filename in files:\n            shutil.move(os.path.join(root, filename), destination)\n        shutil.rmtree(root)\n\ndef download_data(data_folder):\n    train_urls = [\"http://www.statmt.org/wmt13/training-parallel-europarl-v7.tgz\",\n                  \"https://www.statmt.org/wmt13/training-parallel-commoncrawl.tgz\",\n                  \"http://www.statmt.org/wmt14/training-parallel-nc-v9.tgz\",\n                 ]\n\n    # Create a folder to store downloaded TAR files\n    tars_dir = os.path.join(data_folder, \"tar_files\")\n    os.makedirs(tars_dir, exist_ok=True)\n\n    # Create a fresh folder to extract downloaded TAR files; previous extractions deleted to prevent tarfile module errors\n    extracted_dir = os.path.join(data_folder, \"extracted_files\")\n    if os.path.isdir(extracted_dir):\n        shutil.rmtree(extracted_dir)\n        os.mkdir(extracted_dir)\n\n    # Download and extract training data\n    for url in train_urls:\n        filename = url.split(\"/\")[-1]\n\n        train_data_path = os.path.join(data_folder, \"tar_files\", filename)\n\n        if not os.path.exists(train_data_path):\n            print(f\"\\nDownloading {filename}...\")\n            wget.download(url, train_data_path)\n\n        print(f\"\\nExtracting {filename}...\")\n        tar = tarfile.open(train_data_path)\n        members = [m for m in tar.getmembers() if \"de-en\" in m.path]\n        tar.extractall(extracted_dir, members=members)\n\n     # Download validation and testing data using sacreBLEU since we will be using this library to calculate BLEU scores\n    print(\"\\n\")\n    os.system(\"sacrebleu -t wmt13 -l en-de --echo src > '\" + os.path.join(data_folder, \"val.en\") + \"'\")\n    os.system(\"sacrebleu -t wmt13 -l en-de --echo ref > '\" + os.path.join(data_folder, \"val.de\") + \"'\")\n    print(\"\\n\")\n    os.system(\"sacrebleu -t wmt14/full -l en-de --echo src > '\" + os.path.join(data_folder, \"test.en\") + \"'\")\n    os.system(\"sacrebleu -t wmt14/full -l en-de --echo ref > '\" + os.path.join(data_folder, \"test.de\") + \"'\")\n\n    flatten(extracted_dir)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"data_folder=\"/kaggle/working/transformer_data\"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"download_data(data_folder)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Prepare data","metadata":{}},{"cell_type":"code","source":"def prepare_data(data_folder,\n                 euro_parl=True,\n                 common_crawl=True,\n                 news_commentary=True,\n                 retain_case=True):\n    # Read raw files and combine\n    german = list()\n    english = list()\n    files = list()\n\n    assert euro_parl or common_crawl or news_commentary, \"Set at least one dataset to True!\"\n\n    if euro_parl: files.append(\"europarl-v7.de-en\")\n    if common_crawl: files.append(\"commoncrawl.de-en\")\n    if news_commentary: files.append(\"news-commentary-v9.de-en\")\n\n    extracted_dir = os.path.join(data_folder, \"extracted_files\")\n\n    print(\"\\nReading extracted files and combining...\")\n    for file in files:\n        with codecs.open(os.path.join(extracted_dir, file + \".de\"), \"r\", encoding=\"utf-8\") as f:\n            if retain_case:\n                german.extend(f.read().split(\"\\n\"))\n            else:\n                german.extend(f.read().lower().split(\"\\n\"))\n\n        with codecs.open(os.path.join(extracted_dir, file + \".en\"), \"r\", encoding=\"utf-8\") as f:\n            if retain_case:\n                english.extend(f.read().split(\"\\n\"))\n            else:\n                english.extend(f.read().lower().split(\"\\n\"))\n\n        assert len(english) == len(german)\n\n    # Write to file so stuff can be freed from memory\n    print(\"\\nWriting to single files...\")\n    with codecs.open(os.path.join(data_folder, \"train.en\"), \"w\", encoding=\"utf-8\") as f:\n        f.write(\"\\n\".join(english))\n    with codecs.open(os.path.join(data_folder, \"train.de\"), \"w\", encoding=\"utf-8\") as f:\n        f.write(\"\\n\".join(german))\n    with codecs.open(os.path.join(data_folder, \"train.ende\"), \"w\", encoding=\"utf-8\") as f:\n        f.write(\"\\n\".join(english + german))\n    del english, german # free some RAM\n\n    bpe_model_path = os.path.join(data_folder, \"bpe.model\")\n\n    # Perform BPE\n    print(\"\\nLearning BPE...\")\n    yttm.BPE.train(data=os.path.join(data_folder, \"train.ende\"),\n                           vocab_size=37000,\n                           model=bpe_model_path)\n\n    # Load BPE model\n    print(\"\\nLoading BPE model...\")\n    bpe_model = yttm.BPE(model=bpe_model_path)\n\n    return bpe_model, bpe_model_path","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"bpe_model, bpe_model_path = prepare_data(data_folder)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"bpe_model.vocab_size()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"bpe_model.encode(['There was no ring on his finger.',\n                  'That was a good sign although far from proof that he was available.'],\n                 output_type=yttm.OutputType.SUBWORD,\n                 bos=True, eos=True)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Filter data","metadata":{}},{"cell_type":"code","source":"def filter(tokenizer,\n           data_folder,\n           min_length=3,\n           max_length=100,\n           max_length_ratio=1.5):\n    \n    # Re-read English, German\n    print(\"\\nRe-reading single files...\")\n    with codecs.open(os.path.join(data_folder, \"train.en\"), \"r\", encoding=\"utf-8\") as f:\n        english = f.read().split(\"\\n\")\n    with codecs.open(os.path.join(data_folder, \"train.de\"), \"r\", encoding=\"utf-8\") as f:\n        german = f.read().split(\"\\n\")\n\n    # Filter\n    print(\"\\nFiltering...\")\n    pairs = list()\n    for en, de in tqdm(zip(english, german), total=len(english)):\n        en_tok = tokenizer.encode(en, output_type=yttm.OutputType.ID)\n        de_tok = tokenizer.encode(de, output_type=yttm.OutputType.ID)\n\n        len_en_tok = len(en_tok)\n        len_de_tok = len(de_tok)\n\n        if min_length < len_en_tok < max_length and \\\n                min_length < len_de_tok < max_length and \\\n                1. / max_length_ratio <= len_de_tok / len_en_tok <= max_length_ratio:\n            pairs.append((en, de))\n        else:\n            continue\n\n    print(\"\\nNote: %.2f per cent of en-de pairs were filtered out based on sub-word sequence length limits.\" % (100. * (\n            len(english) - len(pairs)) / len(english)))\n\n    english, german = zip(*pairs)\n\n    print(\"\\nRe-writing filtered sentences to single files...\")\n    os.remove(os.path.join(data_folder, \"train.en\"))\n    os.remove(os.path.join(data_folder, \"train.de\"))\n    os.remove(os.path.join(data_folder, \"train.ende\"))\n\n    with codecs.open(os.path.join(data_folder, \"train.en\"), \"w\", encoding=\"utf-8\") as f:\n        f.write(\"\\n\".join(english))\n    with codecs.open(os.path.join(data_folder, \"train.de\"), \"w\", encoding=\"utf-8\") as f:\n        f.write(\"\\n\".join(german))\n\n    del english, german, pairs","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"filter(bpe_model, data_folder)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Zip prepared data","metadata":{}},{"cell_type":"code","source":"!zip -r transformer_data.zip  transformer_data -x \"transformer_data/extracted_files/*\" \"transformer_data/tar_files/*\"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from IPython.display import FileLink\nFileLink(r'transformer_data.zip')","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Unzip prepared data","metadata":{}},{"cell_type":"code","source":"!unzip -o transformer_data.zip -d /","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-18T20:35:05.541602Z","iopub.execute_input":"2024-11-18T20:35:05.541970Z","iopub.status.idle":"2024-11-18T20:35:27.642604Z","shell.execute_reply.started":"2024-11-18T20:35:05.541938Z","shell.execute_reply":"2024-11-18T20:35:27.641348Z"}},"outputs":[{"name":"stdout","text":"Archive:  transformer_data.zip\n   creating: /kaggle/working/transformer_data/\n  inflating: /kaggle/working/transformer_data/val.en  \n  inflating: /kaggle/working/transformer_data/train.de  \n  inflating: /kaggle/working/transformer_data/bpe.model  \n  inflating: /kaggle/working/transformer_data/train.en  \n  inflating: /kaggle/working/transformer_data/test.en  \n  inflating: /kaggle/working/transformer_data/val.de  \n  inflating: /kaggle/working/transformer_data/test.de  \n   creating: /transformer_data/\n  inflating: /transformer_data/val.en  \n  inflating: /transformer_data/train.de  \n  inflating: /transformer_data/bpe.model  \n  inflating: /transformer_data/train.en  \n  inflating: /transformer_data/test.en  \n  inflating: /transformer_data/val.de  \n  inflating: /transformer_data/test.de  \n","output_type":"stream"}],"execution_count":6},{"cell_type":"code","source":"data_folder=\"/kaggle/working/transformer_data\"\nbpe_model_path = os.path.join(data_folder, \"bpe.model\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-18T20:35:27.644658Z","iopub.execute_input":"2024-11-18T20:35:27.644980Z","iopub.status.idle":"2024-11-18T20:35:27.649514Z","shell.execute_reply.started":"2024-11-18T20:35:27.644948Z","shell.execute_reply":"2024-11-18T20:35:27.648602Z"}},"outputs":[],"execution_count":7},{"cell_type":"code","source":"bpe_model = yttm.BPE(model=bpe_model_path)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-18T20:35:27.650614Z","iopub.execute_input":"2024-11-18T20:35:27.650881Z","iopub.status.idle":"2024-11-18T20:35:33.892706Z","shell.execute_reply.started":"2024-11-18T20:35:27.650855Z","shell.execute_reply":"2024-11-18T20:35:33.891721Z"}},"outputs":[],"execution_count":8},{"cell_type":"code","source":"bpe_model.encode(['There was no ring on his finger.',\n                  'That was a good sign although far from proof that he was available.'],\n                 output_type=yttm.OutputType.SUBWORD,\n                 bos=True, eos=True)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-18T20:35:33.894281Z","iopub.execute_input":"2024-11-18T20:35:33.894649Z","iopub.status.idle":"2024-11-18T20:35:33.907298Z","shell.execute_reply.started":"2024-11-18T20:35:33.894620Z","shell.execute_reply":"2024-11-18T20:35:33.906544Z"}},"outputs":[{"execution_count":9,"output_type":"execute_result","data":{"text/plain":"[['<BOS>',\n  '▁There',\n  '▁was',\n  '▁no',\n  '▁ring',\n  '▁on',\n  '▁his',\n  '▁fing',\n  'er.',\n  '<EOS>'],\n ['<BOS>',\n  '▁That',\n  '▁was',\n  '▁a',\n  '▁good',\n  '▁sign',\n  '▁although',\n  '▁far',\n  '▁from',\n  '▁proof',\n  '▁that',\n  '▁he',\n  '▁was',\n  '▁available.',\n  '<EOS>']]"},"metadata":{}}],"execution_count":9},{"cell_type":"markdown","source":"# Transformer implementation","metadata":{}},{"cell_type":"code","source":"class PositionalEncoding(nn.Module):\n    def __init__(\n        self,\n        emb_size,\n        dropout,\n        maxlen=5000\n    ):\n        super(PositionalEncoding, self).__init__()\n        den = torch.exp(- torch.arange(0, emb_size, 2) * math.log(10000) / emb_size)\n        pos = torch.arange(0, maxlen).reshape(maxlen, 1)\n\n        pos_embedding = torch.zeros((maxlen, emb_size))\n        pos_embedding[:, 0::2] = torch.sin(pos * den)\n        pos_embedding[:, 1::2] = torch.cos(pos * den)\n        pos_embedding = pos_embedding.unsqueeze(-2)\n\n        self.dropout = nn.Dropout(dropout)\n        self.register_buffer('pos_embedding', pos_embedding)\n\n    def forward(self, token_embedding):\n        return self.dropout(token_embedding + self.pos_embedding[:token_embedding.size(0), :])\n\nclass Translator(nn.Module):\n    def __init__(\n            self,\n            num_encoder_layers,\n            num_decoder_layers,\n            embed_size,\n            num_heads,\n            src_vocab_size,\n            tgt_vocab_size,\n            dim_feedforward,\n            dropout\n        ):\n        super(Translator, self).__init__()\n\n        # Output of embedding must be equal (embed_size)\n        self.src_embedding = nn.Embedding(src_vocab_size, embed_size)\n        self.tgt_embedding = nn.Embedding(tgt_vocab_size, embed_size)\n\n        self.pos_enc = PositionalEncoding(embed_size, dropout)\n\n        self.transformer = nn.Transformer(\n            d_model=embed_size,\n            nhead=num_heads,\n            num_encoder_layers=num_encoder_layers,\n            num_decoder_layers=num_decoder_layers,\n            dim_feedforward=dim_feedforward,\n            dropout=dropout,\n            batch_first=True,\n        )\n\n        self.ff = nn.Linear(embed_size, tgt_vocab_size)\n\n        self._init_weights()\n\n    def _init_weights(self):\n        for p in self.parameters():\n            if p.dim() > 1:\n                nn.init.xavier_uniform_(p)\n\n    def forward(self, src, trg, src_mask, tgt_mask, src_padding_mask, tgt_padding_mask, memory_key_padding_mask):\n        src_emb = self.pos_enc(self.src_embedding(src))\n        tgt_emb = self.pos_enc(self.tgt_embedding(trg))\n\n        # Shape:\n        #     - src:  (N, S, E) if batch_first=True.\n        #     - tgt: (N, T, E) if batch_first=True.\n        #     - src_mask: (S, S) or (N * num_heads, S, S).\n        #     - tgt_mask: (T, T) or (N * num_heads, T, T).\n        #     - memory_mask: (T, S).\n        #     - src_key_padding_mask: (N, S).\n        #     - tgt_key_padding_mask: (N, T).\n        #     - memory_key_padding_mask: (N, S).\n\n        outs = self.transformer(\n            src=src_emb,\n            tgt=tgt_emb,\n            src_mask=src_mask,\n            tgt_mask=tgt_mask,\n            memory_mask=None,\n            src_key_padding_mask=src_padding_mask,\n            tgt_key_padding_mask=tgt_padding_mask,\n            memory_key_padding_mask=memory_key_padding_mask\n        )\n\n        return self.ff(outs)\n\n    def encode(self, src, src_mask):\n        embed = self.src_embedding(src)\n        pos_enc = self.pos_enc(embed)\n        return self.transformer.encoder(pos_enc, src_mask)\n\n    def decode(self, tgt, memory, tgt_mask):\n        embed = self.tgt_embedding(tgt)\n        pos_enc = self.pos_enc(embed)\n        return self.transformer.decoder(pos_enc, memory, tgt_mask)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-18T20:35:42.876034Z","iopub.execute_input":"2024-11-18T20:35:42.876377Z","iopub.status.idle":"2024-11-18T20:35:42.890195Z","shell.execute_reply.started":"2024-11-18T20:35:42.876345Z","shell.execute_reply":"2024-11-18T20:35:42.889211Z"}},"outputs":[],"execution_count":10},{"cell_type":"markdown","source":"# Init Dataloaders ","metadata":{}},{"cell_type":"code","source":"class SequenceLoader(object):\n    \"\"\"\n    An iterator for loading batches of data into the transformer model.\n\n    For training:\n\n        Each batch contains tokens_in_batch target language tokens (approximately),\n        target language sequences of the same length to minimize padding and therefore memory usage,\n        source language sequences of very similar (if not the same) lengths to minimize padding and therefore memory usage.\n        Batches are also shuffled.\n\n    For validation and testing:\n\n        Each batch contains just a single source-target pair, in the same order as in the files from which they were read.\n    \"\"\"\n\n    def __init__(self, tokenizer, data_folder, source_suffix, target_suffix, split, tokens_in_batch):\n        \"\"\"\n        :param data_folder: folder containing the source and target language data files\n        :param source_suffix: the filename suffix for the source language files\n        :param target_suffix: the filename suffix for the target language files\n        :param split: train, or val, or test?\n        :param tokens_in_batch: the number of target language tokens in each batch\n        \"\"\"\n        self.tokens_in_batch = tokens_in_batch\n\n        self.source_suffix = source_suffix\n        self.target_suffix = target_suffix\n\n        assert split.lower() in {\"train\", \"val\",\"test\"}, \"'split' must be one of 'train', 'val', 'test'! (case-insensitive)\"\n        self.split = split.lower()\n\n        # Is this for training?\n        self.for_training = self.split == \"train\"\n\n        # Load BPE model\n        self.bpe_model = tokenizer\n\n        # Load data\n        with codecs.open(os.path.join(data_folder, \".\".join([split, source_suffix])), \"r\", encoding=\"utf-8\") as f:\n            source_data = f.read().split(\"\\n\")[:-1]\n        with codecs.open(os.path.join(data_folder, \".\".join([split, target_suffix])), \"r\", encoding=\"utf-8\") as f:\n            target_data = f.read().split(\"\\n\")[:-1]\n        assert len(source_data) == len(target_data), \"There are a different number of source or target sequences!\"\n        \n        source_lengths = [len(s) for s in self.bpe_model.encode(source_data, bos=False, eos=False)]\n\n        # target language sequences have <BOS> and <EOS> tokens\n        target_lengths = [len(t) for t in self.bpe_model.encode(target_data, bos=True, eos=True)]\n\n        self.data = list(zip(source_data, target_data, source_lengths, target_lengths))\n\n        # If for training, pre-sort by target lengths - required for itertools.groupby() later\n        if self.for_training:\n            self.data.sort(key=lambda x: x[3])\n\n        # Create batches\n        self.create_batches()\n\n    def create_batches(self):\n        \"\"\"\n        Prepares batches for one epoch.\n        \"\"\"\n\n        print(\"Creating batches\")\n\n        # If training\n        if self.for_training:\n            # Group or chunk based on target sequence lengths\n            chunks = [list(g) for _, g in groupby(self.data, key=lambda x: x[3])]\n\n            # Create batches, each with the same target sequence length\n            self.all_batches = list()\n            for chunk in chunks:\n                # Sort inside chunk by source sequence lengths, so that a batch would also have similar source sequence lengths\n                chunk.sort(key=lambda x: x[2])\n                # How many sequences in each batch? Divide expected batch size (i.e. tokens) by target sequence length in this chunk\n                seqs_per_batch = self.tokens_in_batch // chunk[0][3]\n                # Split chunk into batches\n                self.all_batches.extend([chunk[i: i + seqs_per_batch] for i in range(0, len(chunk), seqs_per_batch)])\n\n            # Shuffle batches\n            shuffle(self.all_batches)\n            self.n_batches = len(self.all_batches)\n            self.current_batch = -1\n        else:\n            # Simply return once pair at a time\n            self.all_batches = [[d] for d in self.data]\n            self.n_batches = len(self.all_batches)\n            self.current_batch = -1\n\n    def __iter__(self):\n        \"\"\"\n        Iterators require this method defined.\n        \"\"\"\n        return self\n\n    def __len__(self):\n      return len(self.all_batches)\n\n    def __next__(self):\n        \"\"\"\n        Iterators require this method defined.\n\n        :returns: the next batch, containing:\n            source language sequences, a tensor of size (N, encoder_sequence_pad_length)\n            target language sequences, a tensor of size (N, decoder_sequence_pad_length)\n            true source language lengths, a tensor of size (N)\n            true target language lengths, typically the same as decoder_sequence_pad_length as these sequences are bucketed by length, a tensor of size (N)\n        \"\"\"\n        # Update current batch index\n        self.current_batch += 1\n        try:\n            source_data, target_data, source_lengths, target_lengths = zip(*self.all_batches[self.current_batch])\n        # Stop iteration once all batches are iterated through\n        except IndexError:\n            raise StopIteration\n\n        # Tokenize using BPE model to word IDs\n        source_data = self.bpe_model.encode(source_data, output_type=yttm.OutputType.ID,\n                                            bos=False, eos=False)\n        target_data = self.bpe_model.encode(target_data, output_type=yttm.OutputType.ID,\n                                            bos=True, eos=True)\n\n        pad_id = self.bpe_model.subword_to_id('<PAD>')\n\n        # Convert source and target sequences as padded tensors\n        source_data = pad_sequence(sequences=[torch.LongTensor(s) for s in source_data],\n                                   batch_first=True,\n                                   padding_value=pad_id)\n        target_data = pad_sequence(sequences=[torch.LongTensor(t) for t in target_data],\n                                   batch_first=True,\n                                   padding_value=pad_id)\n\n        # Convert lengths to tensors\n        # source_lengths = torch.LongTensor(source_lengths)\n        # target_lengths = torch.LongTensor(target_lengths)\n\n        return source_data, target_data","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-18T20:35:46.049485Z","iopub.execute_input":"2024-11-18T20:35:46.049820Z","iopub.status.idle":"2024-11-18T20:35:46.066207Z","shell.execute_reply.started":"2024-11-18T20:35:46.049791Z","shell.execute_reply":"2024-11-18T20:35:46.065200Z"}},"outputs":[],"execution_count":11},{"cell_type":"code","source":"def generate_square_subsequent_mask(size, device):\n    mask = (torch.triu(torch.ones((size, size), device=device)) == 1).transpose(0, 1)\n    mask = mask.float().masked_fill(mask == 0, float('-inf')).masked_fill(mask == 1, float(0.0))\n    return mask\n\n# Create masks for input into model\ndef create_mask(src, tgt, pad_idx, device):\n\n    # Get sequence length\n    src_seq_len = src.shape[1]\n    tgt_seq_len = tgt.shape[1]\n\n    # Generate the mask\n    tgt_mask = generate_square_subsequent_mask(tgt_seq_len, device)\n    src_mask = torch.zeros((src_seq_len, src_seq_len), device=device).type(torch.bool)\n\n    # Overlay the mask over the original input\n    src_padding_mask = (src == pad_idx)\n    tgt_padding_mask = (tgt == pad_idx)\n    tgt_padding_mask = (tgt_padding_mask.float()\n                                        .masked_fill(tgt_padding_mask == 1, float('-inf'))\n                                        .masked_fill(tgt_padding_mask == 0, float(0.0)))\n\n    return src_mask, tgt_mask, src_padding_mask, tgt_padding_mask","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-18T21:30:13.084609Z","iopub.execute_input":"2024-11-18T21:30:13.084983Z","iopub.status.idle":"2024-11-18T21:30:13.091955Z","shell.execute_reply.started":"2024-11-18T21:30:13.084950Z","shell.execute_reply":"2024-11-18T21:30:13.090974Z"}},"outputs":[],"execution_count":104},{"cell_type":"code","source":"def get_data(opts, tokenizer):\n    # Initialize data-loaders\n    train_dataloader = SequenceLoader(tokenizer,\n                                      data_folder=\"/kaggle/working/transformer_data\",\n                                      source_suffix=opts.src,\n                                      target_suffix=opts.tgt,\n                                      split=\"train\",\n                                      tokens_in_batch=opts.tokens_in_batch)\n\n    valid_dataloader = SequenceLoader(tokenizer,\n                                      data_folder=\"/kaggle/working/transformer_data\",\n                                      source_suffix=opts.src,\n                                      target_suffix=opts.tgt,\n                                      split=\"val\",\n                                      tokens_in_batch=opts.tokens_in_batch)\n\n    vocab = tokenizer.vocab()\n\n    src_lang_transform = lambda src_lang: tokenizer.encode(src_lang,\n                                                           output_type=yttm.OutputType.ID,\n                                                           bos=False, eos=False)\n    tgt_lang_transform = lambda tgt_lang: tokenizer.encode(tgt_lang,\n                                                           output_type=yttm.OutputType.ID,\n                                                           bos=True, eos=True)\n\n    special_symbols = {\n        \"<unk>\": tokenizer.subword_to_id('<UNK>'),\n        \"<pad>\": tokenizer.subword_to_id('<PAD>'),\n        \"<bos>\": tokenizer.subword_to_id('<BOS>'),\n        \"<eos>\": tokenizer.subword_to_id('<EOS>'),\n    }\n\n    return train_dataloader, valid_dataloader, vocab, src_lang_transform, tgt_lang_transform, special_symbols","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-18T21:30:14.789818Z","iopub.execute_input":"2024-11-18T21:30:14.790652Z","iopub.status.idle":"2024-11-18T21:30:14.797874Z","shell.execute_reply.started":"2024-11-18T21:30:14.790613Z","shell.execute_reply":"2024-11-18T21:30:14.796924Z"}},"outputs":[],"execution_count":105},{"cell_type":"code","source":"class Opts:\n    def __init__(self):\n        self.src = \"en\"\n        self.tgt = \"de\"\n        self.batch = 128\n        self.tokens_in_batch = 2000\n\nopts = Opts()\n\ntrain_dl, valid_dl, vocab, src_lang_transform, tgt_lang_transform, special_symbols = get_data(opts, bpe_model)\n\nprint(f\"vocab size: {len(vocab)}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-18T21:30:15.886605Z","iopub.execute_input":"2024-11-18T21:30:15.886958Z","iopub.status.idle":"2024-11-18T21:32:22.889875Z","shell.execute_reply.started":"2024-11-18T21:30:15.886926Z","shell.execute_reply":"2024-11-18T21:32:22.888913Z"}},"outputs":[{"name":"stdout","text":"Creating batches\nCreating batches\nvocab size: 37000\n","output_type":"stream"}],"execution_count":106},{"cell_type":"markdown","source":"# Parameters","metadata":{}},{"cell_type":"code","source":"DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n\n# Training settings\nepochs = 5\nlr = 1e-4\nbetas=(0.9, 0.98)\neps=1e-9\n\n# Transformer settings\nattn_heads = 8\nenc_layers = 5\ndec_layers = 5\nembed_size = 512\ndim_feedforward = 512\ndropout = 0.1\n\nbest_val_loss = 1e6\nstart_epoch = 1  # start at this epoch\n\ndry_run = True\n\nlogging_dir = f\"/kaggle/working/{str(date.today())}/\"\nos.makedirs(logging_dir, exist_ok=True)\n\nphases = ['train', 'val']\nsaved_epoch_losses = {phase: [] for phase in phases}\nsaved_bleu_metrics = {phase: [] for phase in phases}","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-18T21:35:05.496078Z","iopub.execute_input":"2024-11-18T21:35:05.496496Z","iopub.status.idle":"2024-11-18T21:35:05.502872Z","shell.execute_reply.started":"2024-11-18T21:35:05.496466Z","shell.execute_reply":"2024-11-18T21:35:05.501968Z"}},"outputs":[],"execution_count":107},{"cell_type":"code","source":"def train(model, train_dl, loss_fn, optim, special_symbols):\n    model.train()\n    \n    losses = 0\n    running_corrects = 0\n    all_elems_count = 0\n\n    curr_tqdm = tqdm(train_dl, total=len(train_dl), ascii=True)\n    for batch_idx, (src, tgt) in enumerate(curr_tqdm):\n        # if batch_idx == 10: break\n        \n        src = src.to(DEVICE)\n        tgt = tgt.to(DEVICE)\n\n        tgt_input = tgt[:, :-1]\n        \n        src_mask, tgt_mask, src_padding_mask, tgt_padding_mask = create_mask(src, tgt_input, special_symbols[\"<pad>\"], DEVICE)\n\n        # (B, T, vocab_size)\n        logits = model(src, tgt_input, src_mask, tgt_mask, src_padding_mask, tgt_padding_mask, src_padding_mask)\n\n        optim.zero_grad()\n\n        tgt_out = tgt[:, 1:]\n\n        flattened_logits = logits.reshape(-1, logits.shape[-1])\n        flattened_tgt = tgt_out.reshape(-1)\n        symbols_count = flattened_tgt.shape[0] \n\n        all_elems_count += symbols_count\n        \n        probs = F.softmax(flattened_logits, dim=1)\n        preds = torch.argmax(probs, dim=1)\n        corrects_cnt = torch.sum(preds == flattened_tgt.detach())\n        running_corrects += corrects_cnt\n        \n        loss = loss_fn(flattened_logits, flattened_tgt)\n        loss.backward()\n\n        optim.step()\n\n        losses += loss.item()\n\n        curr_tqdm.set_postfix({\"Loss\": f\"{loss.item():.2f}\",\n                               \"Corrects\": f\"{corrects_cnt.item()}/{symbols_count}\",\n                               \"Accuracy\": f\"{(corrects_cnt * 100 / symbols_count).item():.3f}%\"})\n\n    epoch_loss_per_batch = losses / len(train_dl)\n    epoch_acc_per_symbol = running_corrects.float().item() / all_elems_count\n    \n    return epoch_loss_per_batch, epoch_acc_per_symbol","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-18T21:35:58.611852Z","iopub.execute_input":"2024-11-18T21:35:58.612734Z","iopub.status.idle":"2024-11-18T21:35:58.620885Z","shell.execute_reply.started":"2024-11-18T21:35:58.612698Z","shell.execute_reply":"2024-11-18T21:35:58.620019Z"}},"outputs":[],"execution_count":114},{"cell_type":"code","source":"def greedy_decode(model, src, src_mask, max_len, start_symbol, end_symbol):\n    src = src.to(DEVICE)\n    src_mask = src_mask.to(DEVICE)\n\n    memory = model.encode(src, src_mask)\n    \n    ys = torch.ones(1, 1).fill_(start_symbol).type(torch.long).to(DEVICE)\n\n    for _ in range(max_len - 1):\n\n        memory = memory.to(DEVICE)\n        tgt_mask = (generate_square_subsequent_mask(ys.size(1), DEVICE).type(torch.bool)).to(DEVICE)\n\n        out = model.decode(ys, memory, tgt_mask)\n\n        # Covert to probabilities and take the max of these probabilities\n        prob = model.ff(out[:, -1])\n        _, next_word = torch.max(prob, dim=1)\n        next_word = next_word.item()\n\n        # Now we have an output which is the vector representation of the translation\n        ys = torch.cat([ys, torch.ones(1, 1).type_as(src.data).fill_(next_word)], dim=1)\n        if next_word == end_symbol:\n            break\n\n    return ys","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-18T21:36:00.084500Z","iopub.execute_input":"2024-11-18T21:36:00.085214Z","iopub.status.idle":"2024-11-18T21:36:00.091945Z","shell.execute_reply.started":"2024-11-18T21:36:00.085176Z","shell.execute_reply":"2024-11-18T21:36:00.090934Z"}},"outputs":[],"execution_count":115},{"cell_type":"code","source":"def translate(src):\n    num_tokens = src.shape[1]\n\n    src_mask = (torch.zeros(num_tokens, num_tokens)).type(torch.bool)\n\n    tgt_tokens = greedy_decode(\n        model, src, src_mask, max_len=num_tokens+5, \n        start_symbol=special_symbols[\"<bos>\"], \n        end_symbol=special_symbols[\"<eos>\"]\n    ).flatten()\n\n    output_as_list = list(tgt_tokens.cpu().numpy())\n    \n    output_list_words = bpe_model.decode([output_as_list], ignore_ids=[0, 2, 3])\n\n    translation = \" \".join(output_list_words)\n\n    return translation","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-18T21:36:00.910372Z","iopub.execute_input":"2024-11-18T21:36:00.910711Z","iopub.status.idle":"2024-11-18T21:36:00.916413Z","shell.execute_reply.started":"2024-11-18T21:36:00.910679Z","shell.execute_reply":"2024-11-18T21:36:00.915413Z"}},"outputs":[],"execution_count":116},{"cell_type":"code","source":"def validate(model, valid_dl, loss_fn, special_symbols):\n    losses = 0\n\n    model.eval()\n\n    hypotheses = list()\n    references = list()\n    \n    for batch_idx, (src, tgt) in enumerate(tqdm(valid_dl, total=len(valid_dl), ascii=True)):\n        # if batch_idx == 10: break\n        \n        src = src.to(DEVICE)\n        tgt = tgt.to(DEVICE)\n\n        tgt_input = tgt[:, :-1]\n\n        src_mask, tgt_mask, src_padding_mask, tgt_padding_mask = create_mask(src, tgt_input, special_symbols[\"<pad>\"], DEVICE)\n\n        logits = model(src, tgt_input, src_mask, tgt_mask,src_padding_mask, tgt_padding_mask, src_padding_mask)\n\n        tgt_out = tgt[:, 1:]\n\n        loss = loss_fn(logits.reshape(-1, logits.shape[-1]), tgt_out.reshape(-1))\n        losses += loss.item()\n\n        translation = translate(src)\n        # print(f\"{translation=}\")\n        \n        hypotheses.append(translation)\n        references.extend(bpe_model.decode(tgt.tolist(), ignore_ids=[0, 2, 3]))\n\n    print(hypotheses[:10])\n    print(references[:10])\n\n    sacrebleu_metrics = {\n        \"13a cased\": sacrebleu.corpus_bleu(hypotheses, [references]),\n        \"13a caseless\": sacrebleu.corpus_bleu(hypotheses, [references], lowercase=True),\n        \"intl cased\": sacrebleu.corpus_bleu(hypotheses, [references], tokenize='intl'),\n        \"intl caseless\": sacrebleu.corpus_bleu(hypotheses, [references], tokenize='intl', lowercase=True)\n    }\n    \n    return losses / len(valid_dl), sacrebleu_metrics","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-18T21:36:02.265314Z","iopub.execute_input":"2024-11-18T21:36:02.266009Z","iopub.status.idle":"2024-11-18T21:36:02.274099Z","shell.execute_reply.started":"2024-11-18T21:36:02.265975Z","shell.execute_reply":"2024-11-18T21:36:02.273209Z"}},"outputs":[],"execution_count":117},{"cell_type":"code","source":"def save_checkpoint(epoch, \n                    model, \n                    optim, \n                    best_val_loss, \n                    saved_epoch_losses, \n                    saved_bleu_metrics,\n                    prefix=''):\n    \"\"\"\n    Checkpoint saver. Each save overwrites previous save.\n\n    :param epoch: epoch number (0-indexed)\n    :param model: transformer model\n    :param optimizer: optimized\n    :param prefix: checkpoint filename prefix\n    \"\"\"\n    state = {'epoch': epoch,\n             'model': model.state_dict(),\n             'optimizer': optim.state_dict(),\n             'best_val_loss': best_val_loss,\n             'saved_epoch_losses': saved_epoch_losses,\n             'saved_bleu_metrics': saved_bleu_metrics}\n\n    filename = prefix + 'transformer_checkpoint.pth'\n\n    torch.save(state, filename)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-18T21:35:08.576053Z","iopub.execute_input":"2024-11-18T21:35:08.576392Z","iopub.status.idle":"2024-11-18T21:35:08.581776Z","shell.execute_reply.started":"2024-11-18T21:35:08.576362Z","shell.execute_reply":"2024-11-18T21:35:08.580847Z"}},"outputs":[],"execution_count":112},{"cell_type":"code","source":"checkpoint = None # \"{logging_dir}/{\"best\" or \"last\"}_transformer_checkpoint.pth.tar\"\n\nmodel = Translator(\n    num_encoder_layers=enc_layers,\n    num_decoder_layers=dec_layers,\n    embed_size=embed_size,\n    num_heads=attn_heads,\n    src_vocab_size=len(vocab),\n    tgt_vocab_size=len(vocab),\n    dim_feedforward=dim_feedforward,\n    dropout=dropout\n).to(DEVICE)\n\n# These special values are from the \"Attention is all you need\" paper\noptim = torch.optim.Adam(\n    model.parameters(),\n    lr=lr,\n    betas=betas,\n    eps=eps\n)\n\n# Initialize model or load checkpoint\nif checkpoint is not None:\n    checkpoint = torch.load(checkpoint, weights_only=True, map_location=DEVICE)\n    print(f'\\nLoaded checkpoint from epoch {start_epoch}.')\n\n    start_epoch = checkpoint['epoch']\n    best_val_loss = checkpoint['best_val_loss']\n    saved_epoch_losses = checkpoint['saved_epoch_losses']\n    saved_bleu_metrics = checkpoint['saved_bleu_metrics']\n\n    model.load_state_dict(checkpoint['model'])\n    optim.load_state_dict(checkpoint['optimizer'])","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-18T21:36:18.525540Z","iopub.execute_input":"2024-11-18T21:36:18.526441Z","iopub.status.idle":"2024-11-18T21:36:19.751645Z","shell.execute_reply.started":"2024-11-18T21:36:18.526392Z","shell.execute_reply":"2024-11-18T21:36:19.750915Z"}},"outputs":[],"execution_count":118},{"cell_type":"code","source":"# Set up our learning tools\nloss_fn = torch.nn.CrossEntropyLoss(ignore_index=special_symbols[\"<pad>\"]).to(DEVICE)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-18T21:36:19.752964Z","iopub.execute_input":"2024-11-18T21:36:19.753239Z","iopub.status.idle":"2024-11-18T21:36:19.757474Z","shell.execute_reply.started":"2024-11-18T21:36:19.753213Z","shell.execute_reply":"2024-11-18T21:36:19.756483Z"}},"outputs":[],"execution_count":119},{"cell_type":"code","source":"for idx, epoch in enumerate(range(start_epoch, epochs + 1)):\n    start_time = time.time()\n    train_dl.create_batches()\n    train_loss, train_acc_per_symbol = train(model, train_dl, loss_fn, optim, special_symbols)\n    epoch_time = time.time() - start_time\n\n    valid_dl.create_batches()\n    val_loss, sacrebleu_metrics  = validate(model, valid_dl, loss_fn, special_symbols)\n\n    # Once training is done, we want to save out the model\n    if val_loss < best_val_loss:\n        best_val_loss = val_loss\n        save_checkpoint(epoch, model, optim, best_val_loss, saved_epoch_losses, saved_bleu_metrics, prefix=(logging_dir + \"best_\"))\n\n    saved_epoch_losses['train'].append(train_loss)\n    saved_epoch_losses['val'].append(val_loss)\n    saved_bleu_metrics['val'].append(sacrebleu_metrics)\n    \n    save_checkpoint(epoch, model, optim, best_val_loss, saved_epoch_losses, saved_bleu_metrics, prefix=(logging_dir + \"last_\"))\n    \n    print(f\"Epoch: {epoch}, \"\n          f\"Train acc per symbol: {train_acc_per_symbol:.2f}, \"\n          f\"Train loss: {train_loss:.3f}, \"\n          f\"Val loss: {val_loss:.3f},\\n\"\n          f\"Epoch time = {epoch_time:.1f} seconds, \"\n          f\"ETA = {epoch_time*(epochs-idx-1):.1f} seconds\")\n\n    print(\"\\n\".join(list(map(lambda k: f\"{k[0]}: {k[1]}\", sacrebleu_metrics.items()))))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-18T21:36:27.325780Z","iopub.execute_input":"2024-11-18T21:36:27.326646Z","iopub.status.idle":"2024-11-18T22:06:51.118997Z","shell.execute_reply.started":"2024-11-18T21:36:27.326608Z","shell.execute_reply":"2024-11-18T22:06:51.117806Z"}},"outputs":[{"name":"stdout","text":"Creating batches\n","output_type":"stream"},{"name":"stderr","text":" 13%|#3        | 7974/59476 [30:20<3:15:58,  4.38it/s, Loss=7.51, Corrects=70/1904, Accuracy=3.676%]  \n","output_type":"stream"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","Cell \u001b[0;32mIn[120], line 4\u001b[0m\n\u001b[1;32m      2\u001b[0m start_time \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()\n\u001b[1;32m      3\u001b[0m train_dl\u001b[38;5;241m.\u001b[39mcreate_batches()\n\u001b[0;32m----> 4\u001b[0m train_loss, train_acc_per_symbol \u001b[38;5;241m=\u001b[39m \u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_dl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mloss_fn\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptim\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mspecial_symbols\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      5\u001b[0m epoch_time \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime() \u001b[38;5;241m-\u001b[39m start_time\n\u001b[1;32m      7\u001b[0m valid_dl\u001b[38;5;241m.\u001b[39mcreate_batches()\n","Cell \u001b[0;32mIn[114], line 42\u001b[0m, in \u001b[0;36mtrain\u001b[0;34m(model, train_dl, loss_fn, optim, special_symbols)\u001b[0m\n\u001b[1;32m     38\u001b[0m     loss\u001b[38;5;241m.\u001b[39mbackward()\n\u001b[1;32m     40\u001b[0m     optim\u001b[38;5;241m.\u001b[39mstep()\n\u001b[0;32m---> 42\u001b[0m     losses \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mitem\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     44\u001b[0m     curr_tqdm\u001b[38;5;241m.\u001b[39mset_postfix({\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mLoss\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mloss\u001b[38;5;241m.\u001b[39mitem()\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.2f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m     45\u001b[0m                            \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCorrects\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mcorrects_cnt\u001b[38;5;241m.\u001b[39mitem()\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00msymbols_count\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m     46\u001b[0m                            \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAccuracy\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m(corrects_cnt\u001b[38;5;250m \u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;250m \u001b[39m\u001b[38;5;241m100\u001b[39m\u001b[38;5;250m \u001b[39m\u001b[38;5;241m/\u001b[39m\u001b[38;5;250m \u001b[39msymbols_count)\u001b[38;5;241m.\u001b[39mitem()\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.3f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m%\u001b[39m\u001b[38;5;124m\"\u001b[39m})\n\u001b[1;32m     48\u001b[0m epoch_loss_per_batch \u001b[38;5;241m=\u001b[39m losses \u001b[38;5;241m/\u001b[39m \u001b[38;5;28mlen\u001b[39m(train_dl)\n","\u001b[0;31mKeyboardInterrupt\u001b[0m: "],"ename":"KeyboardInterrupt","evalue":"","output_type":"error"}],"execution_count":120},{"cell_type":"code","source":"save_checkpoint(epoch, model, optim, best_val_loss, saved_epoch_losses, saved_bleu_metrics, prefix=(logging_dir + \"last_\"))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-18T22:07:15.542360Z","iopub.execute_input":"2024-11-18T22:07:15.543323Z","iopub.status.idle":"2024-11-18T22:07:17.811040Z","shell.execute_reply.started":"2024-11-18T22:07:15.543289Z","shell.execute_reply":"2024-11-18T22:07:17.809992Z"}},"outputs":[],"execution_count":121},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}